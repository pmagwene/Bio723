\documentclass{beamer}

\input{../../preamble/slidespreamble.tex}

\usepackage{tikz}
\usepackage{marvosym} % for male/female symbols
\usepackage{MnSymbol} % for thinstar

\setbeamersize{description width=2em}

\DeclareMathOperator{\rank}{rank} 

%===========================================================
% Title Info
\title{Scientific Computing for Biologists}
\subtitle{Singular Value Decomposition and Biplots}

\author{Instructor: Paul M. Magwene}


\date{02 October 2012}

\begin{document}
%===========================================================
\begin{frame}
\titlepage
\end{frame}

%===========================================================
\begin{frame}
  \frametitle{Overview of Lecture}
  
\begin{itemize}
		\item Singular Value Decomposition
		\begin{itemize}
			\item Algebra of SVD
			\item Geometry of SVD
			\item Relationship to Eigendecomposition
			\item Applications of SVD			
		\end{itemize}		
		\item Biplots
		\begin{itemize}
			\item Simultaneous representation of rows and columns of a matrix
		\end{itemize}			
\end{itemize}

\end{frame}
%===========================================================

%===========================================================
\begin{frame}
  \frametitle{Hands-on Session}
\begin{itemize}
    \item SVD and Biplots in R
    \item Applications of SVD in R
    		\begin{itemize}
    		\item `Seriation' using SVD
			  \item Matrix approximation and image compression using SVD
		\end{itemize}
\end{itemize} 


\end{frame}		
%===========================================================


%===========================================================
\begin{frame}[fragile]
  \frametitle{Matrix Decomposition}

A matrix decomposition is a factoring of a matrix into simpler parts.

\medskip
Some familiar factorizations:

\begin{align*}
12 &= 3 \times 4 = 2 \times 6  \\
x^2 - 4 &= (x-2)(x+2)
\end{align*}


\begin{center}
   \alert{Matrix decomposition is the same idea!}
\end{center}



\end{frame}
%===========================================================



%===========================================================
\begin{frame}
  \frametitle{Eigendecomposition}


You've already been introduced to one way to decompose a square matrix, $\Mtx{A}$:

$$ \Mtx{A} = \Mtx{V}\Mtx{D}\InvMtx{V} $$

where:
\begin{itemize}
\item  $\Mtx{V}$ is a matrix of eigenvectors (in columns)
\item $\Mtx{D}$ is a diagonal matrix with eigenvalues along diagonal.
\end{itemize}

\medskip
when $\Mtx{A}$ is real-valued and symmetric than $\Mtx{V}$ is orthgonal.

\end{frame}
%===========================================================

%===========================================================
% \begin{frame}
%   \frametitle{Singular Value Decomposition}


% %$$ \Mtx{A} = \Mtx{U}\Mtx{S}\Mtx{V}^T $$

% \begin{center}
% \includegraphics[height=3in]{svd-graphic}
% \end{center}

% \end{frame}
%===========================================================

%===========================================================
\begin{frame}
  \frametitle{Singular Value Decomposition}

\includegraphics[height=2in]{fig-svd-overview}

\smallskip
\begin{itemize}
  \item When written like this $\Mtx{U}$ and $\Mtx{V}$ are orthonormal.

  \item Sometimes SVD is written as:
\[
\underset{(n \times p)}{\Mtx{A}} = \underset{(n \times p)}{\Mtx{U}} \underset{(p \times p)}{\Mtx{S}} \underset{(p \times p)}{\Mtx{V}^T} 
\]

\end{itemize}


\end{frame}
%===========================================================



%===========================================================
\begin{frame}
  \frametitle{Facts about SVD}

\begin{itemize}
\item Singular Value Decomposition is often referred to as giving the ``basic structure'' of a matrix

\item The rank of $\Mtx{A}$ is equivalent to the number of non-zero singular values in $ \Mtx{A} = \Mtx{U}\Mtx{S}\Mtx{V}^T $

$$ \rank(\Mtx{A}) \leq \min(n,p) $$


\item  The Euclidean norm ($L_2$) norm of a matrix is the relative amount it stretches a vector:

$$ |\Mtx{A}|_E = \frac{|\Mtx{A}\Mtx{x}|}{|\Mtx{x}|} $$

The $L_2$ norm of $\Mtx{A}$ is given by $\Mtx{S}_{11}$.
\end{itemize}



\end{frame}
%===========================================================

%===========================================================
\begin{frame}
  \frametitle{Geometric Interpretation of SVD}

Any matrix, $\Mtx{A}_{n \times p}$, represents a linear transformation from $\RealP \mapsto  \RealN$. 

\medskip
SVD can be thought of decomposing the transformation specified by $\Mtx{A}$ into a simple form:

$$ \Mtx{A} = (\text{rotation})(\text{scaling)}(\text{rotation})$$

\begin{itemize}
\item $\Mtx{U}$ and $\Mtx{V}$ are orthonormal (orthogonal) matrices $\rightsquigarrow$ Orthonormal matrices represent rigid rotations (or rotation plus reflection).
\item Diagonal matrices represent ``stretching''
\end{itemize}


\end{frame}

%===========================================================


%===========================================================
% \begin{frame}
%   \frametitle{SVD Example}

% \begin{center}
% \includegraphics[height=1.25in]{svd-example-mtx}
% \end{center}

% \medskip
% \textbf{Geometry}
% \begin{center}
% \includegraphics[height=1.25in]{svd-example}
% \end{center}

% \end{frame}
%===========================================================


%===========================================================
\begin{frame}[fragile]
  \frametitle{SVD Example}

\[
\Mtx{A} =  \begin{bmatrix}3 & 1 \\ 2 & 2\end{bmatrix} = \Mtx{U} \Mtx{S} \Mtx{V}^T
\]
where
\[
\Mtx{U} = \begin{bmatrix} -0.75 & -0.66 \\ -0.66 & 0.75 \end{bmatrix}, \:
\Mtx{S} = \begin{bmatrix} 4.13 & 0 \\ 0 & 0.97 \end{bmatrix}, \:
\Mtx{V}^T = \begin{bmatrix} -0.86 & -0.50 \\ 0.50 & -0.86 \end{bmatrix}
\]

\medskip
\alert{Geometry}
\centerline{
\includegraphics[height=1.5in]{fig-svd-example}
}

\end{frame}
%===========================================================


% %===========================================================
% \begin{frame}
%   \frametitle{Relationship of SVD to Eigendecomposition}

% \begin{center}
% \includegraphics[height=2in]{svd-reln-eigen}
% \end{center}

% \end{frame}
% %===========================================================


%===========================================================
\begin{frame}[fragile]
  \frametitle{Relationship of SVD to Eigendecomposition}

\begin{align}
  \Mtx{A} &= \Mtx{U} \Mtx{S} \Mtx{V}^T \\
  \Mtx{A}^ T\Mtx{A} &= (\Mtx{V}\Mtx{S}\Mtx{U}^T)(\Mtx{U} \Mtx{S} \Mtx{V}^T)\\
      &= \Mtx{V} \Mtx{S} \Mtx{U}^T \Mtx{U} \Mtx{S} \Mtx{V}^T \\
      &= \Mtx{V}\Mtx{S}\Mtx{S}\Mtx{V}^T  \label{eq:ortho}
\end{align}
Equation~\ref{eq:ortho} follows from the fact that $\Mtx{U}$ is orthonormal ($\Mtx{U}^T \Mtx{U} = \Mtx{I}$)

\smallskip
If we let $\Mtx{D} = \Mtx{S}\Mtx{S}$, we can rewrite equation~\ref{eq:ortho} as:
\begin{align}
  \Mtx{A}^ T\Mtx{A} &= \Mtx{V}\Mtx{D}\Mtx{V} && \text{\footnotesize Eigendecomposition!}
\end{align}

\begin{itemize}
\item The singlular values $\Mtx{S}_{ii}$ are $\sqrt{\Mtx{D}_{ii}}$ where $\Mtx{d}_{ii}$ are the eigenvalues of $\Mtx{A}^ T\Mtx{A}$.

\item The columns of $\Mtx{V}$ are the eigenvectors of $\Mtx{A}^ T\Mtx{A}$.
\end{itemize}

\end{frame}
%===========================================================

%===========================================================
% \begin{frame}
%   \frametitle{Using SVD to do PCA}

% \begin{center}
% \includegraphics[height=2.5in]{svd-pca}
% \end{center}

% \end{frame}
%===========================================================


%===========================================================
\begin{frame}[fragile]
  \frametitle{Using SVD to do PCA}

Let \Mtx{X} be a mean-centered $n \times p$ data matrix. The covariance matrix is given by:
\begin{align*}
  \Mtx{C} = \tfrac{1}{n-1} \; \Mtx{X}^T \Mtx{X}
\end{align*}

By SVD we can write $\Mtx{X} = \Mtx{U}\Mtx{S}\Mtx{V}^T$, therefore:
\begin{align*}
  \Mtx{C} &= \tfrac{1}{n-1} \; \Mtx{V}\Mtx{S}\Mtx{U}^T \Mtx{U}\Mtx{S}\Mtx{V}^T \\
          &= \tfrac{1}{n-1} \; \Mtx{V} \Mtx{S}\Mtx{S} \Mtx{V}^T
\end{align*}

\begin{itemize}
  \item The PC vectors are given by the columns of \Mtx{V} (rows of $\Mtx{V}^T$)
  \item The PC scores are given by $\Mtx{U}\Mtx{D}$, where $\Mtx{D} = \Mtx{S}\Mtx{S}$
\end{itemize}

\end{frame}
%===========================================================




%===========================================================
\begin{frame}
  \frametitle{Another Way of Thinking about SVD}


\begin{center}
\includegraphics[width=\textwidth]{fig-svdpca}
\end{center}

\end{frame}

%===========================================================


%===========================================================
\begin{frame}
  \frametitle{Applications of SVD}

\begin{itemize}
\item Pseudoinverse of an arbirary matrix
\item Matrix approximation
\item Biplots
\item ... and many more ...
\end{itemize}

\end{frame}
%===========================================================


%===========================================================
\begin{frame}
  \frametitle{Pseudoinverse via SVD}

The pseudoinverse of a matrix is a generalization of the concept of a matrix inverse. Only square matrices have a matrix inverse; the pseudoinverse applies to an arbitrary $n \times p$ matrix.

\smallskip
Given an $n \times p$ matrix $\Mtx{A}$ find matrix $\PsInv{A}$ such that:

\begin{eqnarray*}
\Mtx{A} \PsInv{A} \Mtx{A} & = & \Mtx{A} \\
\PsInv{A} \Mtx{A} \PsInv{A} & = & \PsInv{A}\\
(\Mtx{A} \PsInv{A})^T & = & \Mtx{A} \PsInv{A}\\
(\PsInv{A} \Mtx{A})^T & = & \PsInv{A} \Mtx{A}
\end{eqnarray*}

\smallskip
Moore-Penrose Inverse via SVD:

\begin{eqnarray*}
\text{if}\ \Mtx{A} &=& \Mtx{U}\Mtx{S}\Mtx{V}^T \\
\PsInv{A} &=& \Mtx{V} \PsInv{S}  \Mtx{U}^T
\end{eqnarray*}

where \PsInv{S} has the reciprocal of non-zero elements of S.

\end{frame}
%===========================================================

%===========================================================
\begin{frame}
  \frametitle{SVD for Matrix Approximation}

If $ \Mtx{A} = \Mtx{U} \Mtx{S} \Mtx{V}^T $
then the optimal (least-squares) $k$-dimensional approximation of \Mtx{A} (where $ k < \rank(\Mtx{A})$) is given by:
\[
\tilde{\Mtx{A}} = \Mtx{U}\Mtx{S}^{\thinstar} \Mtx{V}^T
\]

where:
\begin{eqnarray*}
\Mtx{S}_{ii}^{\thinstar} &=& \Mtx{S}_{ii} \text{ for } i \leq k\\
\Mtx{S}_{ii}^{\thinstar} &=& 0 \text{ for } i > k\\
\end{eqnarray*}

\end{frame}
%===========================================================


% %===========================================================
% \begin{frame}
%   \frametitle{Biplots}


% \begin{center}
% \includegraphics[height=2.75in]{about-biplots}
% \end{center}

% \end{frame}

% %===========================================================


%===========================================================
\begin{frame}
  \frametitle{Biplots}

\begin{itemize}
  \item Technique for simultaneously displaying row and column data (observations and variables)
  \item Invented by K. Gabriel (see also papers by Gower)
\end{itemize}

Given a data matrix, \Mtx{X}, we can use SVD to approximate \Mtx{X} as so:
\begin{gather*}
  \tilde{\Mtx{X}}_k =  \Mtx{U}\Mtx{S}^{\thinstar} \Mtx{V}^T \\
  \text{\footnotesize (k-dimensional approximation to \Mtx{X})}
\end{gather*}


We can rewrite $\tilde{\Mtx{X}}_k$ as a product of two matrices:
\[
  \Mtx{X} = \Mtx{G}\Mtx{H}^T
\]
where
\[
  \Mtx{G} = \Mtx{U}(\Mtx{S}^{\thinstar})^\alpha\; \text{ and } \ \Mtx{H}^T = (\Mtx{S}^{\thinstar})^{1-\alpha}\Mtx{V}^T
\]
\end{frame}

%===========================================================


%===========================================================
\begin{frame}
  \frametitle{Biplots, cont.}

\begin{eqnarray*}
\Mtx{G} &=& \Mtx{U}(\Mtx{S}^{\thinstar})^{\alpha} \text{ (row effects) } \\
\Mtx{H}^T &=& (\Mtx{S}^{\thinstar})^{1-\alpha} \Mtx{V}^T \text{ (columns effects) }
\end{eqnarray*}

Different choices of $\alpha$ emphasize different relationships in the data.

\begin{itemize}
\item \Red{$\alpha = 0$}, column-metric preserving biplot; optimally approximates variance-covariance structure. Cosine of angles between vectors approximate correlations; distances between points approximate Mahalanobis distance ( ``correlation biplot'')
\item \Red{$\alpha = 1$}, row-metric preserving biplot; optimally approximates Euclidean distances among observations. Coordinates of observations correspond to PC scores; coordinates of variables correspond to eigenvector coefficients (``distance biplot''). PCs are `sphered'.
\item \Red{$\alpha = 0.5$},  optimally approximates observational values (``symmetric biplot'')
\end{itemize}

\end{frame}

%===========================================================

%===========================================================
\begin{frame}
  \frametitle{Biplots, Example}

\begin{itemize}
\item Observations drawn as points in space of PCs
\item Variables drawn as vectors in PC space
\end{itemize}

\medskip

\centerline{
\includegraphics[height=1.75in]{fig-irisbiplots.pdf}
}
\begin{center}
PCA Biplots of Iris data set.
\end{center}

\end{frame}

%===========================================================


%===========================================================
\begin{frame}
  \frametitle{Biplots, Example II}


\centerline{
\includegraphics[height=3in]{fig-crabs}
}

\begin{center}
{\footnotesize Biplots for dataset of five morphological measurements on crabs.}
\end{center}


\end{frame}

%===========================================================




\end{document}


%===========================================================
\begin{frame}
  \frametitle{XXX}

\end{frame}
%===========================================================
